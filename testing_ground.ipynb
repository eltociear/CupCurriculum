{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "from typing import Tuple\n",
    "from tensorboardX import SummaryWriter\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Progressbar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Highlevel from Pytorch\n",
    "import torch as T\n",
    "from torch import nn, Tensor\n",
    "import torch.optim as opt\n",
    "\n",
    "# Neural Network parts from Pytorch\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Pytorch's Dataset and Dataloader\n",
    "from torch.utils.data import dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Dataset used\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# Custom Libraries\n",
    "import utils\n",
    "device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "\n",
    "train_iter, test_iter, val_iter = WikiText2.iters(batch_size = batch_size, device = device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 20])\n",
      "torch.Size([35, 20])\n"
     ]
    }
   ],
   "source": [
    "training_batch = next(iter(train_iter))\n",
    "print(training_batch.text.size())\n",
    "print(training_batch.target.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "min_list = []\n",
    "max_list = []\n",
    "for data in train_iter:\n",
    "    for index in range(data.text.size()[1]):\n",
    "        min_list.append(min(data.text[index]))\n",
    "        max_list.append(max(data.text[index]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33278)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(max(max_list))\n",
    "print(min(min_list))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE]\n",
      "                             [--eval_batch_size EVAL_BATCH_SIZE] [--bptt BPTT]\n",
      "                             [--ntokens NTOKENS] [--emsize EMSIZE]\n",
      "                             [--d_hid D_HID] [--nlayers NLAYERS]\n",
      "                             [--nhead NHEAD] [--dropout DROPOUT]\n",
      "                             [--num_prune_cycles NUM_PRUNE_CYCLES]\n",
      "                             [--num_epochs_prune NUM_EPOCHS_PRUNE]\n",
      "                             [--prune_percent PRUNE_PERCENT]\n",
      "                             [--print_freq_prune PRINT_FREQ_PRUNE]\n",
      "                             [--test_freq_prune TEST_FREQ_PRUNE]\n",
      "                             [--num_epochs_reint NUM_EPOCHS_REINT]\n",
      "                             [--print_freq_reint PRINT_FREQ_REINT]\n",
      "                             [--test_freq_reint TEST_FREQ_REINT] [-v]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Luca\\AppData\\Roaming\\jupyter\\runtime\\kernel-3d2e9205-ec99-4d27-a2b4-0315a1ed257c.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[1;31mSystemExit\u001B[0m\u001B[1;31m:\u001B[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luca\\anaconda3\\envs\\MasterThesis\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3406: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use a Parser to specify Hyperparams etc.\n",
    "parser = argparse.ArgumentParser()\n",
    "# TODO: Think about adding the seed or experiment number\n",
    "# TODO: Change the {utils.checkdir(f\"{os.getcwd()}/some/path/\")} expressions to something like\n",
    "# TODO: {utils.checkdir(f\"{os.getcwd()}/some/path/Seed{seed}/\")} or {utils.checkdir(f\"{os.getcwd()}/some/path/Experiment{experiment}/\")}\n",
    "# TODO: Obviously change the save commands as well\n",
    "# Set Hyperparams for Batches\n",
    "parser.add_argument(\"--batch_size\", type=int, default=20, help=\"The Batchsize used for Training\")\n",
    "parser.add_argument(\"--eval_batch_size\", type=int, default=10, help=\"The Batchsize used for Evaluation\")\n",
    "parser.add_argument(\"--bptt\", type=int, default=35, help=\"The Length of Backpropagation through Time\")\n",
    "# Set Hyperparams specifying the Model\n",
    "parser.add_argument(\"--ntokens\", type=int, default=33280, help=\"The Number of Tokens used by the Model\")\n",
    "parser.add_argument(\"--emsize\", type=int, default=200, help=\"The Embedding Dimension used by the Model\")\n",
    "parser.add_argument(\"--d_hid\", type=int, default=200, help=\"The Dimension of the FFN Model used in the Encoder\")\n",
    "parser.add_argument(\"--nlayers\", type=int, default=2, help=\"The Number of Encoderlayers used in the Encoder\")\n",
    "parser.add_argument(\"--nhead\", type=int, default=2, help=\"The Number of Heads used in the Multihead-Attention\")\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.2, help=\"The Dropout Probability used in the Model\")\n",
    "# Set Hyperparams defining the Pruning Procedure\n",
    "# TODO: Think about adding rewind option and number of warmup steps\n",
    "# Facebook Paper uses num_prune_cycles = 20 and prune_percent = 20. as well as 50,000 updates (overall?)\n",
    "parser.add_argument(\"--num_prune_cycles\", type=int, default=20, help=\"The Number of Pruning Cycles\")\n",
    "parser.add_argument(\"--num_epochs_prune\", type=int, default=50, help=\"The Number of Epochs per Pruning Cycle\")\n",
    "parser.add_argument(\"--prune_percent\", type=float, default=20., help=\"The Percentage of remaining Weights to be pruned in each Iteration\")\n",
    "parser.add_argument(\"--print_freq_prune\", type=int, default=1, help=\"The Printing-Frequency of Train- and Test Loss during Pruning\")\n",
    "parser.add_argument(\"--test_freq_prune\", type=int, default=1, help=\"The Testing Frequency during Pruning\")\n",
    "# Set Hyperparams defining the Reintroduction Procedure\n",
    "# TODO: Think about adding choice option (selecting reintroduction scheme)\n",
    "parser.add_argument(\"--num_epochs_reint\", type=int, default=50, help=\"The Number of Epochs per Reintialisation\")\n",
    "parser.add_argument(\"--print_freq_reint\", type=int, default=1, help=\"The Printing Frequency of Train- and Test Loss durinig Reinitialisation\")\n",
    "parser.add_argument(\"--test_freq_reint\", type=int, default=1, help=\"The Testing Frequency during Reinitialisation\")\n",
    "# TODO: Think about adding LR, the Factor used in scheduler, etc.\n",
    "parser.add_argument(\"-v\", \"--verbosity\", action=\"count\", default=0)\n",
    "args = parser.parse_args()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "# Defining the Architecture\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type          = 'Transformer'\n",
    "        self.pos_encoder         = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers           = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder             = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model             = d_model\n",
    "        self.decoder             = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src    = self.encoder(src) * math.sqrt(self.d_model)  # Wordembeddings\n",
    "        src    = self.pos_encoder(src)  # Positional Encoding\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return T.triu(T.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "\n",
    "# Implementing Positional Encoding, i.e. where are the words in the Sentence\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout   = nn.Dropout(p=dropout)\n",
    "        position       = T.arange(max_len).unsqueeze(1)\n",
    "        div_term       = T.exp(T.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe             = T.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = T.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = T.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# Building the Model to be trained\n",
    "model = TransformerModel(33280, 200, 2, 200, 2, 0.2).to(device)\n",
    "# Finished defining the Model\n",
    "\n",
    "# Specify the objective Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr        = 5.0  # learning rate\n",
    "# Stated in Successfully applying ... (Aachen) Adafactor gives better results than Adam, they include warmup after reset\n",
    "optimizer = T.optim.SGD(model.parameters(), lr=lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def warmup(num_warmup: int = 5) -> None:\n",
    "    # Progressbar\n",
    "    bar = tqdm(range(num_warmup))\n",
    "    for epoch in bar:\n",
    "        total_loss   = 0.\n",
    "        comp_loss    = 0.  # Used for comparison down below\n",
    "        log_interval = 200\n",
    "        start_time   = time.time()\n",
    "        src_mask     = generate_square_subsequent_mask(35).to(device)\n",
    "        model.train()  # turn on train mode\n",
    "        for batch_num, batch in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            data_pts, targets = batch.text, batch.target\n",
    "            batch_size_local = data_pts.size(0)\n",
    "            if batch_size_local != 35:  # only on last batch\n",
    "                src_mask = src_mask[:batch_size_local, :batch_size_local]\n",
    "            output = model(data_pts, src_mask)\n",
    "            t_loss = criterion(output.view(-1, 33280), targets.view(output.view(-1, 33280).size()[0]))\n",
    "            t_loss.backward()\n",
    "            # Clipping Gradients\n",
    "            T.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            total_loss += t_loss.item()\n",
    "            if batch_num % log_interval == 0 and batch_num > 0:\n",
    "                ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "                cur_loss = (total_loss - comp_loss) / log_interval\n",
    "                # ppl          = math.exp(cur_loss)\n",
    "                comp_loss = total_loss\n",
    "                print(f'| epoch {epoch:3d} | {batch_num:5d}/{len(train_iter):5d} batches | '\n",
    "                      f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                      f'loss {cur_loss:5.2f}')  # | ppl {ppl:8.2f}')\n",
    "                start_time = time.time()\n",
    "    # Copying and Saving State after Warm-Up\n",
    "    #utils.checkdir(f\"{os.getcwd()}/saves/model_state_dicts/\")\n",
    "    #T.save(model, f\"{os.getcwd()}/saves/model_state_dicts/warmup_state_dict.pth.tar\")\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |   200/ 2984 batches | lr 5.00 | ms/batch 882.09 | loss  8.27\n",
      "| epoch   0 |   400/ 2984 batches | lr 5.00 | ms/batch 954.36 | loss  7.00\n",
      "| epoch   0 |   600/ 2984 batches | lr 5.00 | ms/batch 915.79 | loss  6.57\n",
      "| epoch   0 |   800/ 2984 batches | lr 5.00 | ms/batch 913.77 | loss  6.39\n",
      "| epoch   0 |  1000/ 2984 batches | lr 5.00 | ms/batch 889.67 | loss  6.28\n",
      "| epoch   0 |  1200/ 2984 batches | lr 5.00 | ms/batch 926.50 | loss  6.25\n",
      "| epoch   0 |  1400/ 2984 batches | lr 5.00 | ms/batch 691.58 | loss  6.18\n",
      "| epoch   0 |  1600/ 2984 batches | lr 5.00 | ms/batch 688.07 | loss  6.20\n",
      "| epoch   0 |  1800/ 2984 batches | lr 5.00 | ms/batch 704.14 | loss  6.06\n",
      "| epoch   0 |  2000/ 2984 batches | lr 5.00 | ms/batch 692.15 | loss  6.08\n",
      "| epoch   0 |  2200/ 2984 batches | lr 5.00 | ms/batch 669.33 | loss  5.99\n",
      "| epoch   0 |  2400/ 2984 batches | lr 5.00 | ms/batch 664.36 | loss  6.01\n",
      "| epoch   0 |  2600/ 2984 batches | lr 5.00 | ms/batch 663.57 | loss  5.99\n",
      "| epoch   0 |  2800/ 2984 batches | lr 5.00 | ms/batch 736.24 | loss  5.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [38:46<2:35:04, 2326.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2984 batches | lr 5.00 | ms/batch 715.18 | loss  5.92\n",
      "| epoch   1 |   400/ 2984 batches | lr 5.00 | ms/batch 707.80 | loss  5.87\n",
      "| epoch   1 |   600/ 2984 batches | lr 5.00 | ms/batch 703.15 | loss  5.72\n",
      "| epoch   1 |   800/ 2984 batches | lr 5.00 | ms/batch 688.56 | loss  5.73\n",
      "| epoch   1 |  1000/ 2984 batches | lr 5.00 | ms/batch 690.31 | loss  5.70\n",
      "| epoch   1 |  1200/ 2984 batches | lr 5.00 | ms/batch 692.69 | loss  5.72\n",
      "| epoch   1 |  1400/ 2984 batches | lr 5.00 | ms/batch 703.42 | loss  5.74\n",
      "| epoch   1 |  1600/ 2984 batches | lr 5.00 | ms/batch 697.62 | loss  5.77\n",
      "| epoch   1 |  1800/ 2984 batches | lr 5.00 | ms/batch 692.82 | loss  5.66\n",
      "| epoch   1 |  2000/ 2984 batches | lr 5.00 | ms/batch 692.68 | loss  5.71\n",
      "| epoch   1 |  2200/ 2984 batches | lr 5.00 | ms/batch 693.35 | loss  5.61\n",
      "| epoch   1 |  2400/ 2984 batches | lr 5.00 | ms/batch 690.57 | loss  5.66\n",
      "| epoch   1 |  2600/ 2984 batches | lr 5.00 | ms/batch 695.81 | loss  5.66\n",
      "| epoch   1 |  2800/ 2984 batches | lr 5.00 | ms/batch 692.03 | loss  5.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [1:13:23<1:48:59, 2179.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |   200/ 2984 batches | lr 5.00 | ms/batch 693.06 | loss  5.64\n",
      "| epoch   2 |   400/ 2984 batches | lr 5.00 | ms/batch 688.71 | loss  5.63\n",
      "| epoch   2 |   600/ 2984 batches | lr 5.00 | ms/batch 689.21 | loss  5.47\n",
      "| epoch   2 |   800/ 2984 batches | lr 5.00 | ms/batch 684.80 | loss  5.50\n",
      "| epoch   2 |  1000/ 2984 batches | lr 5.00 | ms/batch 685.17 | loss  5.49\n",
      "| epoch   2 |  1200/ 2984 batches | lr 5.00 | ms/batch 684.77 | loss  5.52\n",
      "| epoch   2 |  1400/ 2984 batches | lr 5.00 | ms/batch 682.13 | loss  5.55\n",
      "| epoch   2 |  1600/ 2984 batches | lr 5.00 | ms/batch 683.54 | loss  5.57\n",
      "| epoch   2 |  1800/ 2984 batches | lr 5.00 | ms/batch 680.93 | loss  5.48\n",
      "| epoch   2 |  2000/ 2984 batches | lr 5.00 | ms/batch 680.25 | loss  5.53\n",
      "| epoch   2 |  2200/ 2984 batches | lr 5.00 | ms/batch 677.81 | loss  5.44\n",
      "| epoch   2 |  2400/ 2984 batches | lr 5.00 | ms/batch 684.22 | loss  5.47\n",
      "| epoch   2 |  2600/ 2984 batches | lr 5.00 | ms/batch 675.18 | loss  5.49\n",
      "| epoch   2 |  2800/ 2984 batches | lr 5.00 | ms/batch 676.75 | loss  5.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [1:47:20<1:10:28, 2114.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |   200/ 2984 batches | lr 5.00 | ms/batch 678.29 | loss  5.48\n",
      "| epoch   3 |   400/ 2984 batches | lr 5.00 | ms/batch 676.16 | loss  5.50\n",
      "| epoch   3 |   600/ 2984 batches | lr 5.00 | ms/batch 675.22 | loss  5.33\n",
      "| epoch   3 |   800/ 2984 batches | lr 5.00 | ms/batch 675.80 | loss  5.36\n",
      "| epoch   3 |  1000/ 2984 batches | lr 5.00 | ms/batch 672.54 | loss  5.35\n",
      "| epoch   3 |  1200/ 2984 batches | lr 5.00 | ms/batch 674.46 | loss  5.36\n",
      "| epoch   3 |  1400/ 2984 batches | lr 5.00 | ms/batch 674.71 | loss  5.41\n",
      "| epoch   3 |  1600/ 2984 batches | lr 5.00 | ms/batch 673.34 | loss  5.45\n",
      "| epoch   3 |  1800/ 2984 batches | lr 5.00 | ms/batch 668.65 | loss  5.35\n",
      "| epoch   3 |  2000/ 2984 batches | lr 5.00 | ms/batch 696.57 | loss  5.41\n",
      "| epoch   3 |  2200/ 2984 batches | lr 5.00 | ms/batch 699.95 | loss  5.30\n",
      "| epoch   3 |  2400/ 2984 batches | lr 5.00 | ms/batch 708.91 | loss  5.35\n",
      "| epoch   3 |  2600/ 2984 batches | lr 5.00 | ms/batch 699.69 | loss  5.38\n",
      "| epoch   3 |  2800/ 2984 batches | lr 5.00 | ms/batch 696.97 | loss  5.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [2:21:18<34:44, 2084.50s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |   200/ 2984 batches | lr 5.00 | ms/batch 679.14 | loss  5.39\n",
      "| epoch   4 |   400/ 2984 batches | lr 5.00 | ms/batch 668.24 | loss  5.38\n",
      "| epoch   4 |   600/ 2984 batches | lr 5.00 | ms/batch 669.59 | loss  5.21\n",
      "| epoch   4 |   800/ 2984 batches | lr 5.00 | ms/batch 669.26 | loss  5.26\n",
      "| epoch   4 |  1000/ 2984 batches | lr 5.00 | ms/batch 665.86 | loss  5.25\n",
      "| epoch   4 |  1200/ 2984 batches | lr 5.00 | ms/batch 666.06 | loss  5.29\n",
      "| epoch   4 |  1400/ 2984 batches | lr 5.00 | ms/batch 664.84 | loss  5.32\n",
      "| epoch   4 |  1600/ 2984 batches | lr 5.00 | ms/batch 666.14 | loss  5.35\n",
      "| epoch   4 |  1800/ 2984 batches | lr 5.00 | ms/batch 663.28 | loss  5.31\n",
      "| epoch   4 |  2000/ 2984 batches | lr 5.00 | ms/batch 671.55 | loss  5.32\n",
      "| epoch   4 |  2200/ 2984 batches | lr 5.00 | ms/batch 677.42 | loss  5.22\n",
      "| epoch   4 |  2400/ 2984 batches | lr 5.00 | ms/batch 668.50 | loss  5.27\n",
      "| epoch   4 |  2600/ 2984 batches | lr 5.00 | ms/batch 666.26 | loss  5.27\n",
      "| epoch   4 |  2800/ 2984 batches | lr 5.00 | ms/batch 670.01 | loss  5.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [2:54:34<00:00, 2094.85s/it]\n"
     ]
    }
   ],
   "source": [
    "warmup()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}